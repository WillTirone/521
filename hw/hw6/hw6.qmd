---
title: "STA 521 HW6"
author: "William Tirone"
format: pdf
editor: visual
---

# The Honor Code:

::: callout-important
\(a\) Please state the names of people who you worked with for this homework. You can also provide your comments about the homework here.

\(b\) Please type/write the following sentences yourself and sign at the end. We want to make it extra clear that nobody cheats even unintentionally.

*I hereby state that all of my solutions were entirely in my words and were written by me. I have not looked at another student's solutions and I have fairly credited all external sources in this write up.*
:::

# 1 True or False

1.  ::: {.callout-note appearance="minimal"}
    FALSE, it is built on a random set of *m* predictors.
    :::

2.  ::: {.callout-note appearance="minimal"}
    FALSE, the trees in boosting are grown sequentially.
    :::

3.  ::: {.callout-note appearance="minimal"}
    FALSE, the hyperparameters are the number of trees, the number of features/covariates, and the type of tree.
    :::

4.  ::: {.callout-note appearance="minimal"}
    FALSE, they can handle discrete or continuous-valued covariates.
    :::

5.  ::: {.callout-note appearance="minimal"}
    FALSE, the forest makes the model less interpretable since it's a combination of the individual trees.
    :::

6.  ::: {.callout-note appearance="minimal"}
    TRUE, a single tree could potentially overfit and learning slowly can help avoid this (ISL p. 346).
    :::

7.  ::: {.callout-note appearance="minimal"}
    FALSE, AdaBoost works by giving misclassified points more weight in future iterations.

    Maybe TRUE? Check lecture 23 p. 21 / ESL 10.6
    :::

8.  ::: {.callout-note appearance="minimal"}
    FALSE, a deep tree can overfit and lead to higher variance.
    :::

9.  ::: {.callout-note appearance="minimal"}
    FALSE, in BART trees are grown successively on the original data (lecture 23 slide 29)
    :::

10. ::: {.callout-note .}
    FALSE, AdaBoost is iterative.
    :::

11. ::: {.callout-note .}
    TRUE, deeper trees have higher variance and a higher chance to overfit in general.
    :::

12. ::: {.callout-note .}
    FALSE, it can overfit if you add too many trees.
    :::

# 2)

# 3) 

# 4) 

ISL Book: 8.4.9 (all 11 parts) Page 363-364

```{r}
# set up 
suppressPackageStartupMessages({
library(ISLR2)
library(tree)
})
data(OJ)
```

## a) 

```{r}
set.seed(123)
OJ.train = sample_n(OJ,800)
OJ.test = OJ[-as.integer(rownames(OJ.train)), ]
```

## b)

Training Error Rate: 0.165.

Terminal Nodes: 8

```{r}
t1 = tree(Purchase ~ ., data=OJ.train)
summary(t1)
```

## c) 

The terminal node below indicates the split criteria, number of observations, deviance, y label, and the proportion of observations that that either MM or the other labels in the node.

     9) LoyalCH > 0.0356415 114  108.90 MM ( 0.18421 0.81579 )

```{r}
t1
```

## d) 

Interpretation: starting from the top, we can follow the edges to the terminal nodes to find the label for the training observations. For example, LoyalCH \< 0.0356415 predicts a label of *MM* and LoyalCH \> 0.5036, PriceDiff \> -0.39, and LoyalCH \> 0.705326 predicts a label of *CH*.

```{r}
plot(t1)
text(t1)
```

## e) 

The test error rate is 0.1851

```{r}
t1.pred = predict(t1, OJ.test, type='class')
table(t1.pred, OJ.test$Purchase)

# 1 minus correct classification 
1 - (150 + 70)/270
```

## f) 

A tree with size 5 minimizes the misclassification error at 138.

```{r}
cv.t1 = cv.tree(t1, FUN = prune.misclass)
cv.t1
```

## g) 

```{r}
x = cv.t1$size
y = cv.t1$dev

plot(x,y,type='l',
     xlab='tree size', 
     ylab = 'cross validation misclassification',
     main='Tree Cross Validation on OJ Train Data Set')
```

## h) 

A tree with size 5 minimizes the misclassification error at 138.

## i)

Creating a tree with 5 nodes, the optimal number of nodes:

```{r}
prune.t1 = prune.misclass(t1, best = 5)
plot(prune.t1)
text(prune.t1)
```

## j) 

The pruned tree has a misclassification error rate of 0.165, which is the same as the unpruned version in this case.

```{r}
summary(prune.t1)
```

# 5) 
