---
title: "HW1"
format: html
editor: visual
---

```{r}
library(ggplot2)
```

### helpful links:

https://arxiv.org/pdf/1404.1100.pdf (PCA example)

ISL 12.5 p. 538

# 1 True or false

Examine whether the following statements are true or false and provide one line justification.

\(a\) The data collection process usually has little-to-no influence on the outcome of a predictive modeling problem.

**FALSE.** It has a huge influence! If you wanted to estimate the number of voters supporting Democrats in the next election and only sampled people at a Republican convention, you would have very unreliable results.

\(b\) Eigenvalues obtained from principal component analysis are always nonnegative.

\(c\) The first principal vector and the second principal vector are always orthogonal.

**TRUE**. Since we want the second principal vector to be uncorrelated with the first, we make them orthogonal. (source: p. 501 ISL)

\(d\) The singular values of a square matrix M are the same as the eigenvalues of M.

**FALSE.**Â They are the square roots of the non-zero eigenvalues of $M^TM$ or $MM^T$. (Source: textbook and question #2 of this homework).

\(e\) Principal component analysis can be used to create a low dimensional projection of the

data.

\(f\) Eigenvalues of a matrix are always non-negative.

**FALSE.** Counterexample below:

```{r}
A = matrix(-1,2,2)
eigen(A)
```

\(g\) After applying K-means, the vectors representing the first cluster center and the second

cluster center are always orthogonal.

# 2 SVD

# 3 Power Method

# 4 PCA

**(a) Use apply() function to compute mean and variance of all the four columns**

```{r}
apply(USArrests, MARGIN=2, FUN=mean)
```

```{r}
apply(USArrests, MARGIN=2, FUN=var)
```

**(b) Plot a histogram for each of the four columns**

```{r}
hist(USArrests$Murder)
hist(USArrests$Assault)
hist(USArrests$UrbanPop)
hist(USArrests$Rape)
```

**(c) Do you see any correlations between the four columns? Plot and comment**

It looks like murder and rape are right skewed and urban pop and assault are somewhat normally distributed, though assault looks more bi-modal.

After looking at the plots below it looks like there's some light correlation. I'm not sure if I should iterate and compare the other columns with each other, I imagine that's the point of PCA as that would be tedious for a data set with more than a few columns.

```{r}
ggplot(USArrests, aes(x=Murder,y=Rape)) + geom_point() 
ggplot(USArrests, aes(x=UrbanPop,y=Assault)) + geom_point() 
```

**(d) Use prcomp() function to perform principal component analysis. Make sure you standardized the data matrix. Print a summary at the end.**

```{r}
#?prcomp()
pca_model = prcomp(USArrests,scale = TRUE)

#not sure if this is what we need for "print a summary"
pca_model
```

```{r}
dimnames(pca_model$rotation)
```

**(e) Obtain the principal vectors and store them in a matrix, include row and column names. Display the first three loadings.**

DOUBLE CHECK WHETHER OR NOT WE WANT ROWS OR COLUMNS. I think rows contain the "loadings" and the loading vectors contain all the loadings, thus want 3 rows.

```{r}
# each column below contains the loading vectors 
row_name = c("Murder","Assault","UrbanPop","Rape")  
col_name = c("PC1","PC2","PC3","PC4")
loading_matrix = matrix(pca_model$rotation, 4,4, dimnames=list(row_name,col_name))
loading_matrix[1:3,]
```

**(f) Obtain the principal components (or scores) and store them in a matrix, include row and column names. Display the first three PCs.**

```{r}
#this may be a matrix, might need to remove matrix function on (e)
# pretty sure class(loading_matrix) gives matrix so maybe redo that
scores = pca_model$x 
scores[1:3,]
```

**(g) Obtain the eigenvalues and store them in a vector. Display the entire vector, and compute their sum.**

# 5 Optional
