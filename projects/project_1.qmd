---
title: "Project 1 Redwood Data Report"
author: William Tirone (2774025) and Natalie Smith (2819547)
format: pdf
editor: visual
---

```{r,echo=FALSE,message = FALSE, warning = FALSE}
library(tidyverse)
library(GGally)
library(lubridate)
library(anytime)
```

# TODO

-   revise each other's sections of part 1
-   check on depth scale
-   double check we removed all nulls
-   try ggpairs()?

```{r}
# loading in data
data_net = read.csv('data/sonoma-data-net.csv')
data_log = read.csv('data/sonoma-data-log.csv')
```

# 2 Data Cleaning

```{r}

#data_net file 

hist(data_net$voltage/100,breaks=100) # above outlier here? 
hist(data_net$depth/10,breaks=100) # outlier here 
hist(data_net$humidity,breaks=100) # bottom outlier here , some > 100%
hist(data_net$humid_temp,breaks=100) # above outlier here, temperature
hist(data_net$humid_adj,breaks=100) # max and min outlier 
hist(data_net$hamatop,breaks=100) #unsure on outliers
hist(data_net$hamabot,breaks=100) # also unsure on outliers

#data_log 
hist(data_log$voltage,breaks=100) # min outlier here, change scale
hist(data_log$depth/10,breaks=100)
hist(data_log$humidity,breaks=100)
hist(data_log$humid_temp,breaks=100)
hist(data_log$humid_adj,breaks=100)
hist(data_log$hamatop,breaks=100) 
hist(data_log$hamabot,breaks=100)
```

```{r}
# PART A 

#reading in data 
data_net = read.csv('data/sonoma-data-net.csv')
data_log = read.csv('data/sonoma-data-log.csv')

#scaling voltage and depth to match first data set 
data_net = data_net |> mutate(voltage = voltage/100, depth = depth/10)
data_log = data_log |> mutate(depth = depth/10)
data_all = rbind(data_net,data_log)

#converting to dates from chr to datetime to plot over time 

data_all$result_time = as.POSIXct(data_all$result_time)

# removing dates that don't make sense
data_all = data_all |> filter(month(result_time)<7)

# PART B 

# removing null values and filtering bad dates 
bad_data = data_all |> filter(is.na(humidity))
data_all = data_all |> filter(!is.na(humidity)) 
```

```{r}
# identifying the time range for the data with null values
summary(bad_data$result_time)
```

\(c\)

```{r}
location_data = read.table('data/mote-location-data.txt',header=TRUE)
data_all = dplyr::left_join(data_all,location_data,by=c('nodeid' = 'ID'))
cat("Number of total variables in new df : ",dim(data_all)[2])
```

d\)

# Outlier Rejection Methodology

| Variable   | Notes                                  |
|------------|----------------------------------------|
| humidity   |                                        |
| Humid Temp |                                        |
| hamatop    | incident                               |
| hamabot    | reflected, remove max value, too large |

```{r}
max((data_net$parent)) # largest 16-bit integer, throw out 
max((data_net$depth)) # largest 8-bit 

hist(data_all$humidity,breaks=100) # bottom outlier here , some > 100%
hist(data_all$humid_temp,breaks=100) # above outlier here, temperature
hist(data_all$hamatop,breaks=100) #unsure on outliers
hist(data_all$hamabot,breaks=100) 

data_all %>%
  filter(data_all$humidity>0) |>
  ggplot(aes(x = humidity)) +
  geom_boxplot()


```

```{r}
# using quantiles 
quantile(data_all$voltage,probs = seq(0,1,0.1))
```

```{r}
# outlier removal 

data_all |> filter(voltage >= 2.4 & voltage <= 5.5) # per spec sheet
  
#  filter(humidity >= 0 & humidity <= 100) |> 
```

*(e) (Bonus) Discuss other possible outliers and explain your reason why it is better to remove them than to keep them.*

# 3 Data Exploration

*(a) Make some pairwise scatterplots of some variables. Pick a reasonable time period. Explain your choice and describe your findings.*

```{r}

# columns of interest 
ggpairs(data_all, 5:11, progress = FALSE)
```

*(b) Are any of the predictors associated with Incident PAR? If so, explain the relationship.*

```{r,warning=FALSE}
# drop first 4 id columns 
ggcorr(data_all[,c(-1,-2,-3,-4)],label=TRUE)
```

*(c) Each variable of our data basically have three dimensions: value, height and time. Consider each variable as a time series and look at its temporal trend. Generate such plots (value vs time) with height as color cue for at least four variables (Temperature, Relative Humidity, Incident PAR and Reflected PAR). You can do it for different time scales (during an hour, during a day or during the entire experiment). However, at least the plots with days as x-axis are required. Comment on the range, continuity and strange behaviors in these variables.*

*(d) After PCA analysis, generate scree plot of the data. Can this data be approximated by some low-dimensional representation?*

# 4 Interesting Findings

*Describe two/three interesting findings from exploratory analysis of the data. Try to use the techniques that you have learned, such as histograms, PCA, K-means, GMM and hierachical clustering etc. Note that even though you got a dataframe with only a few columns, you may reshape the dataframe before doing any EDA, such as reorganizing such that aggregated information in each day is a column, or a particular hour in each day is a column. Comment on your interesting findings. Different bonuses are given based on how interesing your result is.*

*(a) Finding 1.*

*(b) Finding 2.*

*(c) (Bonus) Finding 3. Bonus is given only if we find all three findings interesting.*

# 5 Graph Critique

The overall quality of the paper by Tolle et al. is good. However, some plots are not perfect from a statistician's point of view.

(a) Figure 3\[a\] shows the distributions of sensor readings projected onto the value dimension, using a histogram. It turns out that both the incident and reflected PAR have long tail. We could not read full information from this histogram. Try to make a better plot with log transform of the data.

(b) What message do the boxplots in Figure 3\[c\] and 3\[d\] try to convey? Do you think the plots convey the right messages? If not generate a new plot with the same data. Hint: compare to some plots in Figure 4.

(c) Any suggestions for improving the first two plots in Figure 4? Can you distinguish all the colors in these two plots?

(d) Comment on Figure 7. Is it possible to generate a better visualization to highlight the difference between network and log data?
